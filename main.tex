\documentclass[a4paper,11pt,oneside,openany]{jsbook}
\usepackage{graphicx,enumerate}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{longtable}
\usepackage{supertabular}
\usepackage{subfigure}
\usepackage{lscape}
\usepackage{url}
\usepackage{here}
\usepackage[toc,page]{appendix}

% declaration of the new block
\algblock{ParallelForByBlocks}{EndParallelForByBlocks}
% customising the new block
\algnewcommand\algorithmicparallelforbyblocks{\textbf{parallelForByBlocks}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparallelforbyblocks{\textbf{end\ parallelForByBlocks}}
\algrenewtext{ParallelForByBlocks}[1]{\algorithmicparallelforbyblocks\ #1\ \algorithmicpardo}
\algrenewtext{EndParallelForByBlocks}{\algorithmicendparallelforbyblocks}


\algblock{ParallelForByThreads}{EndParallelForByThreads}
\algnewcommand\algorithmicparallelforbythreads{\textbf{parallelForByThreads}}
\algnewcommand\algorithmicendparallelforbythreads{\textbf{end\ parallelForByThreads}}
\algrenewtext{ParallelForByThreads}[1]{\algorithmicparallelforbythreads\ #1\ \algorithmicpardo}
\algrenewtext{EndParallelForByThreads}{\algorithmicendparallelforbythreads}

\pagestyle{plain}
\setlength{\textwidth}{\fullwidth}
\setlength{\evensidemargin}{\oddsidemargin}
\def\vector#1{\mbox{\boldmath $#1$}}
\begin{document}
\thispagestyle{empty}
%------------------------------標題紙作成エリア----------------------------%
2017年度　修士学位論文%1
\bigskip%2
\LARGE%3
\begin{center}
修士学位論文
\end{center}
\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip %7
\begin{center} %8
PDBを用いたGPU上の並列IDA*探索
\end{center}
\large %11
\begin{center}
Parallel IDA* search using PDB for GPUs 
\end{center}
\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip
\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip
\Large %17
\begin{center}
広域システム科学系　広域科学専攻
\end{center}
\Large %17
\begin{center}
指導教員:　福永 アレックス
\end{center}
\LARGE %21
\begin{center}
山村　武史
\end{center}
\normalsize
%---------------------------------目次エリア-------------------------------%
\thispagestyle{empty}
\setcounter{tocdepth}{3}
\tableofcontents
%---------------------------------本文エリア-------------------------------%

\chapter{序論}
\section{研究の概要}
本研究では、ヒューリスティック探索の一つであるIDA*探索\cite{Kor85}をGPUを利用した並列化手法についての研究に取り組んだ。
IDA*探索についてGPUを利用した高速な並列アルゴリズムのひとつにBPIDA*探索\cite{HA17}がある。この手法では、IDA*探索を一つのブロックに対してノードを一つ割り当て、このノードを根とするf値制限深さ優先探索を行うことにより、warp divergenceを隠蔽する。堀江らは15-puzzleベンチマーク問題集においてBPIDA*がCPU上の１スレッド直列実行時と比べて6.78倍ほどのスピードアップを得られると実証した。

% 堀江らの研究\cite{HA17}におけるBPIDA*探索では、ベンチマークとして15-puzzleを使用し、これを評価した。
本研究では、15-puzzleより探索空間の大きな24-puzzleをベンチマークとして使用し、BPIDA*探索の性能を再評価した結果、BPIDA*の性能が先行研究と比べて大幅に劣化することが観測された。15-puzzleでは、直列実行時と比べて6.78倍ほどの高速化であったBPIDA*探索が、24-puzzleでは1.25倍程度の高速化しか得られなかった。

BPIDA*の性能劣化の原因として、共有メモリに大きな領域を確保したため、探索性能が劣化した可能性があると考えられる。
BPIDA*探索ではブロック間で共有する探索状態スタックを64Kbyteの共有メモリ上におく。堀江らの研究では、15-puzzleを解くのに必要なスタックのメモリの大きさは3Kbyte程度であった。
必要なスタックのサイズは状態表現や、解の深さに比例し、問題によってはより大きなメモリを必要とする。
本論文の実験で用いた24-puzzleの問題集を解くのに必要なスタックの大きさは、およそ18Kbyteであった。

共有メモリは64Kbyteの領域をL1キャッシュと共有し、共有メモリの占める領域が大きいと当然L1キャッシュのメモリ領域が縮小される。L1キャッシュには、レジスタピルや、グローバルメモリへのアクセスによる遅延を隠蔽する役割をもつ。従って、必要とする共有メモリ領域が24-puzzleでは15-puzzleの6倍程度になったため、L1キャッシュが十分に機能できず、探索性能が劣化した可能性があると考えられる。

本論文ではこの仮説を検証するため、共有メモリにアルゴリズムの探索挙動に影響を与えない無駄なメモリを確保し、GPUにおける並列IDA*探索にどのような影響が生じるか検証して、L1キャッシュの領域が減ると、探索性能が著しく劣化することを実証する。

次に大きなスタックが共有メモリにあることにより、探索性能が下がることを隠蔽するために、スタックを共有メモリではなく、グローバルメモリにおくBPIDA*GLOBAL探索を本論文では提案する。
グローバルメモリは共有メモリより容量が大きい一方で、アクセスの遅延も大きい。
L1キャッシュの領域が増えることの高速化が、グローバルメモリへのアクセスへの遅延より効果が大きければ、BPIDA*GLOBAL探索はより良い探索性能を示すと考えられる。

性能評価の結果、BPIDA*GLOBAL探索が、24-puzzleの問題集において、CPUにおける直列実行のIDA*探索の4.58倍ほどの高速化が得られると実証する。

BPIDA*GLOBAL探索が、低速なグローバルメモリを利用したにもかかわらず、高速化に成功したことをうけ、ヒューリスティック関数にマンハッタン距離ではなく、パターンデータベースを使用したGPU上の並列IDA*探索を実装、評価する。
パターンデータベースはマンハッタン距離より精度の高いヒューリスティック関数であるが、大きなメモリ領域を必要とする。例えば、本論文で使用した24-puzzle問題のパターンデータベースは255Mbyteほどの容量を必要とした。
これは当然64Kbyteの共有メモリには収まらないので、領域の大きなグローバルメモリに置かなければならない。
グローバルメモリへのアクセスの遅延より、精度の高いヒューリスティック関数を使うことによる高速化の影響が大きければ、マンハッタン距離よりパターンデータベース使用時の方がより良い探索性能を示すことが期待できる。

% 検証評価の結果、グローバルメモリにおいたパターンデータベースを使用したGPU上の並列IDA*探索が、共有メモリにおいたマンハッタン距離を使用した時よりも大幅な高速化が行えることを確認できた。
% またグローバルメモリへのアクセスの遅延の影響を受けないCPUにおける高速化と、並列IDA*探索での高速化の割合を比較することで、この遅延がどれほど探索に影響を与えるか確認できた。

% また先行研究におけるBPIDA*探索\cite{HA17}では、ヒューリスティック関数としてマンハッタン距離を使用し、これを共有メモリにおいて使用していた。
% % 共有メモリは、GPUカーネル側に置くことのできるオンチップメモリであり、カーネル側から高速にアクセスすることができる一方、そのメモリ容量は、64Kbyteと比較的小さい。

% ヒューリスティック探索の探索性能は、ヒューリスティック関数の精度に依存する。N-puzzle問題における、ヒューリスティック関数のうちマンハッタン距離より精度の高いものとして、パターンデータベースがあげられる。パターンデータベースは例えば$N=24$の時のN-puzzle問題では、255Mbyteほどの容量を必要とし、マンハッタン距離に比べ、かなり多くのメモリを使用する必要がある。共有メモリが64Kbyteの記憶領域をL1キャッシュと共有していることを考慮すると、パターンデータベースをBPIDA*探索で使用するには、当然より大きなメモリ領域を使用しなければならない。
% 本研究では、パターンデータベースを共有メモリよりも、容量が大きく、低速なグローバルメモリに置いたBPIDA*探索と、従来の共有メモリにマンハッタン距離をおいたBPIDA*探索で、スピードアップの比較を行った。これにより、共有メモリではなくグローバルメモリを使用することによる遅延がBPIDA*探索の探索時間にどのような変化をしめすのか検証した。

% またこの検証結果を踏まえ、共有メモリに異なる大きさの記憶領域を確保した際に、探索効率がどのように変わるのか、より詳細な検証を行った。



\section{本論文の構成}
本論文は以下の通りに構成される。

2章で、本研究の背景としてGPUおよび、GPU上でのプログラミングに必要なプラットフォームであるCUDAについての説明を行った後、本研究で取り扱うグラフ探索問題について問題の定義と、それを取り扱う古典的なアルゴリズムについて説明する。

3章では、本研究と関連した先行研究について紹介するとともに、先行研究より探索空間の大きな24-puzzleにおいて、直列なIDA*探索とBPIDA*探索を再現した実験について紹介する。

4章では、共有メモリではなく、グローバルメモリにスタックを置いたBPIDA*GLOBAL探索を提案し、また共有メモリが大きな領域を確保した際にGPU上の並列IDA*探索がどのような影響をうけるかその検証結果を紹介する。

5章では、グローバルメモリにPattern Databaseをおいた直列環境および並列環境におけるIDA*探索の実験について紹介する。

6章では、本研究における知見をまとめる。

\chapter{導入}
% \chapter{GPUとグラフ探索アルゴリズム}
\section{GPU}
\subsection{GPUとCUDA}
GPU(graphics processing unit)は2Dや3Dコンピューターグラフィックスの画像処理を行う為に従来使用され、それらの性能を向上することを追求してきた。
GPUをグラフィックスのレンダリングのみならず、より汎用的な計算にも利用することを目的とした技術がGPGPU(General-purpose computing on graphics processing units)である。
GPGPUが使われる分野として、物理シミュレーション、機械学習、音声処理、金融工学などのアプリケーションがあげられる。現在、機械学習への注目が高まるとともに、その需要が高まっている。

CUDA(Compute Unified Device Architecture)とはNVIDIA社が提供するGPGPUのための統合開発環境である。GPUの並列コンピュートエンジンを利用して多くの複雑なコンピューティング問題をより効率的に解決する。CUDAではC、C++、Fortranなどの言語で開発が可能である。
% 本節では、GPUを利用したCUDAプログラミングにおけ、およびメモリモデルについて説明する。

\subsection{CUDAプログラミングの構造}
CUDAプログラミングモデルでは、CPUとそれを補完するGPUの両方を使用したヘテロジニアスコンピューティングを行うことができる。CPUとそのメモリを{\bf ホスト}、GPUとそのメモリを{\bf デバイス}と呼び、デバイス側で実行されるコードのことをカーネルと呼ぶ。ホスト側からカーネル関数を呼ぶことでGPU上での処理をCPUと非同期的に行うことができる。

またCUDAで並列計算を行う際の実行単位は、スレッド、ブロック、グリッドと呼ばれる三つの階層に分けることができる。

スレッドはカーネルを動作させた時のプログラムの最小単位である。CPUではコア数と同数のスレッドしか動作できないのに対し、GPUではコアに対し数千～数万といった数のスレッドを並列に動作できるのが強みである。

ブロックはスレッドの集まりであり、同一ブロック内のスレッド同士での同期、共有メモリによるデータの共有が可能である。一つのブロックに最大512スレッド格納することができ、x方向、y方向、z方向の三次元で、スレッドの配置を指定することができる。

グリッドはブロックの集まりであり、ブロック同様x方向、y方向、z方向の三次元で、ブロックの配置を指定することができる。x方向、y方向に配置できる最大ブロック数は、それぞれ65535個であり、これ以上のブロックを配置することはできない。

CUDAはSIMT(SIngle Instruction Multiple Thread)アーキテクチャを採用しており、スレッドが32個ずつのワープと呼ばれるグループにまとめられ、ワープ内のスレッドは全て同じ命令を同時に実行する。同一ワープ内のスレッドがif文による分岐などで異なる命令を実行することを{\bf ワープダイバージェンス}と呼ぶ。ワープ内のスレッドが異なる分岐パスを選択する場合、各分岐パスを逐次的に実行し、そのパスを通らないスレッドは無効化される。これによってパフォーマンスが大幅に低下する可能性があり、ワープ内の分岐をできる限り揃えることがパフォーマンス向上に不可欠となる。

またローカルなメモリやキャッシュメモリ、レジスタ、演算装置、ワープスケジューラなどの計算資源の集まりをSM(Streaming Multiprocessor)と呼ぶ。SMにはスレッドブロックが割り当てられ、それをさらに32個のワープに分割し、利用可能なハードウェアリソースを割り当てる。

\begin{figure}[H]
\begin{center}
\includegraphics[keepaspectratio, width=8cm, bb=0 0 610 445, scale=0.2]{cuda.png}
\caption{CUDAのアーキテクチャ}
\end{center}
\end{figure}

\subsection{メモリモデル}
プログラミングを行う上でメモリは一般的に、プラグラマー側が明示的に制御可能なプログラマブルなメモリと、データの配置を制御することが不可能なノンプログラマブルなメモリの二つに分類される。CUDAのメモリモデルでは、レジスタ、共有メモリ、ローカルメモリ、コンスタントメモリ、テクスチャメモリ、グローバルメモリの6種類が制御可能なプログラマブルメモリであり、GPUのキャッシュであるL1キャッシュ、L2キャッシュ、コンスタントキャッシュ、テクスチャキャッシュの4種類がノンプログラマブルなメモリである。

またCUDAにおけるメモリはさらにGPUの内部に存在するオンチップメモリと、オフチップメモリに分かれる。
オンチップメモリでは高速なアクセスが可能であるが、容量が小さい。一方、オフチップメモリではアクセスは低速だが、その容量は大きいという特徴がある。
ここでは、本論文において重要な意味をもつグローバルメモリ、共有メモリ、L1キャッシュ、レジスタについて詳しく説明する。

グローバルメモリは、オフチップメモリの一つで、GPUにおいて最も容量が大きく、最も遅延が大きいメモリである。グローバルメモリでは、すべてのスレッド、ホストから読み書きが可能であり、SMの外部に用意されている。

共有メモリは、オンチップメモリの一つで、一つのブロック内の全てのスレッドが共有する記憶領域であり、グローバルメモリと比べて、GPU側からの高速な読み書きが可能となっている。一方その容量は比較的小さく、一つのブロックあたり、L1キャッシュと共有した64Kbyteの領域しか確保できない。

L1キャッシュはノンプログラマブルメモリの一つで、ローカルメモリとグローバルメモリにアクセス時のデータをキャッシュするために使用される。そのメモリ容量は共有メモリと64Kbyteのメモリ領域を共有する。

レジスタはGPUにおいて最も高速なオンチップメモリであり、カーネルで宣言される自動変数は通常レジスタに登録される。レジスタの数がハードウェア制限を超える場合、超えた分のレジスタはローカルメモリに退避される。このような現象をレジスタピルと呼び、パフォーマンスに悪影響を及ぼす可能性がある。レジスタピルが起きた際、ローカルメモリからのロードは、L1キャッシュに保存されることがあるため、カーネルが使用するレジスタの数が多い場合、L1キャッシュの量を増やすと効果的である。

CUDAプログラミングモデルにおいて、これらのメモリ領域の容量、レイテンシの大小やスコープを意識したアルゴリズムを実装することが、性能を引き出す上で重要である。

% \section{CUDA}
% CUDA(Compute Unified Device Architecture)とはNVIDIA社が提供するGPUコンピューティングのための統合開発環境である。。GPUの並列コンピュートエンジンを利用して多くの複雑なコンピューティング問題をより効率的に解決する。CUDAではC、C++、Fortranなどの言語で開発が可能である。本節ではCUDAプログラミングにおける

% \subsection{GPUのメモリ構造}
% GPUの大きな特徴の一つとしてCPUとは、異なるメモリ構造があげられる。GPUのデバイス側で使われるメモリは大きくGPU側のオンチップメモリと、CPU側のオフチップメモリの2つに別れる。オンチップメモリの方が、カーネル側からは高速なアクセスが可能である一方、そのサイズは小さくなる。大容量であるメモリのほとんどは、CPU側のオンチップメモリに存在し、容量と引き換えに、高遅延となっている。
% 変数のスコープやライフタイムについても、それぞれのメモリで異なり、これらの特徴をまとめると、以下の図のようになっている。GPUを使用したプログラミングでは、これらのメモリの特徴を踏まえた上で、適したメモリを選択することが、高速化の肝となる。
% 特にグローバルメモリと共有メモリについては本論文において重要な役割を果たすため、紹介する。
% グローバルメモリはCPU上において最も容量が大きい一方で遅延が最も大きいメモリである。すべてのスレッドとホストの両方から読み書きが可能である。
% それに対して、共有メモリはオンチップメモリの一つであり、同じブロック同士でのみスコープを共有する。


% \subsection{CUDA}
% CUDA(Compute Unified Device Architecture)とはNVIDIA社が提供する汎用的な並列コンピューティングプラットフォームとAPIを含む、プログラミングモデルであり、NVIDIAによって開発された。GPUの並列コンピュートエンジンを利用して多くの複雑なコンピューティング問題をより効率的に解決する。CUDAではC、C++、Fortranなどの言語で開発が可能であり、
% CUDAではCPUとそのメモリを扱うホストと、GPUとそのメモリを扱うデバイスの二つにわかれている、ヘテロジニアスアーキテクチャである。

% \subsection{実行単位}
% CUDAプログラミングの特徴の一つとして
% \begin{enumerate}
% \item　ホスト
% \vspace{3mm}
% \newline
% ぶらぶらぶらぶらぶら
% \newline

% \item　デバイス
% \vspace{3mm}
% \newline
% ぶらぶらぶらぶら
% \end{enumerate}

% \newpage
\section{グラフ探索アルゴリズム}
\subsection{グラフ探索問題}
グラフ$G$とはノードの集合$V$と、そのノード同士を結ぶ集合$E$からなり、そのようなグラフを$G=(V,E)$と表す。
$e \in E$は2つのノード$v \in V$の列$e = (v_{1}, v_{2})$で表される。
ある$e \in E$に対して$e = (v_{1}, v_{2})$と$e = (v_{2}, v_{1})$を区別しない場合、そのようなグラフを無向グラフと呼び、一方区別する場合、有向グラフと呼ぶ。
また、それぞれの辺$e = (v_{1}, v_{2})$はコスト$c$を持つ。

グラフ探索問題の入力は、このグラフ$G=(V,E)$と、初期ノード$v_0$とゴールノード$v_g$である。それに対し、出力は、初期ノード$v_0$からゴールノード$v_g$へ至るまでの経路、つまり辺の列$\langle(v_{1}, v_{2}), (v_{2}, v_{3}), ...,(v_{k-1}, v_{g}) \rangle$である。

本節では、このグラフ探索問題を解く古典的なアルゴリズムについていくつか紹介する。

\subsection{A*探索}
A*探索\cite{HNR68}はヒューリスティック関数と呼ばれる、ある状態からゴール状態までの見積もりを道しるべに使用したグラフ探索の手法の一つである。
% 状態$v$について、初期状態から現在までの移動コスト$g(v)$と、現在から終了状態までのコストの見積もり値$h(v)$の合計$f(v) = g(v) + h(v)$が小さなノードから順に探索を行うことで、ゴール状態までの経路を効率良く探索することができる。

A*探索では、すでに展開され、再び展開する必要のないノードをクローズドリスト、これから展開する必要のあるノードをオープンリストに保存する。一度評価した状態および、未展開の状態を全て記録するため、重複展開を避けることができる。

オープンリストはプライオリティーキューとなっており、状態の評価値$f(v)$が小さなノードから順に展開される。状態の評価値$f(v)$は初期状態から現在までの移動コスト$g(v)$と、現在から終了状態までのコストの見積もり値$h(v)$の和$f(v) = g(v) + h(v)$で決定される。

$h(v)$はヒューリスティック関数によって計算される。$h(v)$が、実際のある状態から、ゴール状態に至るまでの最短移動コスト以下であるとき、このようなヒューリスティック関数をadmissibleであると呼ぶ。

admissibleなヒューリスティック関数を使用した場合、A*探索では必ず最短経路を発見することが保証される。
また本研究で使用するManhattan DistanceやPattern Data Baseはadmissibleなヒューリスティックである。

A*探索の探索効率は、このヒューリスティック関数に依存し、ヒューリスティック関数が常に現在からゴール状態までの最短コストを返す場合、このようなヒューリスティクスをパーフェクトヒューリスティクスと呼ぶ。
またヒューリスティック値$h(v)$が常に0である場合、A*探索はダイクストラ法と同じアルゴリズムとなる。
A*探索は適切なヒューリスティック関数を用いることでダイクストラ法よりも効率良く、最短経路問題を解くことが可能である。

A*探索の問題点として、メモリ使用量の大きさがあげられる。答えとなる最短経路の深さを$d$、各ノードを展開した際の子ノードの平均の数を$b$(branching factor)とした場合、A*探索の空間計算量は$O(b^d)$となる。これは探索で生成した全ての状態をメモリに保存するため、指数的に大きなメモリを必要とするためである。
このような問題点を解決する手法として、後述のIDA*探索\cite{Kor85}、メモリが不足するとf値の大きなノードをオープンリストから消去して探索を行うSMA*探索\cite{Rus92}などがある。


\newpage
\begin{algorithm}
\caption{A*探索}
\label{alg:pbnf}
\begin{algorithmic}[1]
\State $openList$を見積もり値$f(v)$を優先度としたプライオリティーキューとして定義
\State 初期ノード$v_0$を$openList$に追加。
\State $closedList$をハッシュテーブルとして定義
\While {$openList$が空ではない}
    \State $v_{cur}$ $\leftarrow$ $openList$の中で $f(v)$が最も小さいノード
    \State $openList$から$v_{cur}$を削除
    \If {$v_{cur} \in 終了状態$}
        \State $v_0$から$v_{cur}$までの遷移と、移動コスト$g(v_{cur})$を解として出力して終了
    \EndIf
    \State $closedList$に$v_{cur}$を追加
    \For{each $v_{next}$ $\in$ $children(v_{cur})$ do}
        \State $f_{new}(v_{next}) \leftarrow g(v_{cur}) + cost(v_{cur}, v_{next}) + h(v_{next})$
        \If {$v_{next}\not\in openList \land v_{next}\not\in closedList$}
            \State $f(v_{next}) \leftarrow f_{new}(v_{next})$ 
            \State $openList$に$v_{next}$を追加
            \State $v_{next}$の親を$v_{cur}$として記録
        \EndIf
        \If {$v_{next}\in openList \land f_{new}(v_{next}) < f(v_{next})$}
            \State $f(v_{next}) \leftarrow f_{new}(v_{next})$
            \State $openList$から$v_{next}$を削除し、再び$v_{next}$を追加することで、優先度を更新
            \State $v_{next}$の親を$v_{cur}$として記録
        \EndIf
        \If {$v_{next}\in closedList \land f_{new}(v_{next}) < f(v_{next})$}
            \State $f(v_{next}) \leftarrow f_{new}(v_{next})$
            \State $closedList$から$openList$に$v_{next}$を移動
            \State $v_{next}$の親を$v_{cur}$として記録
        \EndIf
    \EndFor
\EndWhile
\State 初期状態から終了状態までのパスは存在しないと出力して終了
\end{algorithmic}
\end{algorithm}
\newpage

\subsection{IDA*探索}
IDA*探索\cite{Kor85}はKorfによって提案された手法であり、そのアルゴリズムは反復深化深さ優先探索(IDDFS)にA*探索を応用したものである。

IDDFSでは深さを制限した深さ制限深さ優先探索を繰り返すことによって最短経路を求めることができる。IDA*探索は、この深さ制限深さ優先探索を行う代わりに、ヒューリスティック関数を用いたゴールまでの下限値$h(v)$と、その状態までの深さ$g(v)$の和$f(v) = g(v) + h(v)$によるf値制限深さ優先探索を行う。f値制限深さ優先探索のf値を徐々に大きくしながら繰り返すことでゴール状態までの経路を発見することができる。仮に$h(v)$が常に0である時、IDA*探索はIDDFSと同義になる。
またA*探索同様、admissibleなヒューリスティック関数を用いることで、最初に発見した経路が最短経路であることが保証される。

IDA*探索の特徴として、メモリ使用量があげられる。空間計算量は答えとなる最短経路の深さを$d$、各ノードを展開した際の子ノードの平均の数$b$(branching factor)とした時、A*探索では、$O(b^d)$であったのに対し、IDA*探索では$O(bd)$となる。
これはA*探索が全ての探索済みの状態を保持したまま探索を進めるのに対し、IDA*探索では、クローズドリストを用いず重複展開を許容するためである。よってIDA*探索では、探索の深さに対して線形のメモリを保つことができ、A*探索ではメモリ不足で解けなかった問題においても解を発見することが可能である。
一方、IDA*探索の欠点として、何度も同じ状態を展開するため、A*探索と比べて無駄な探索を行う点があげられる。

\newpage
\begin{algorithm}
\caption{IDA*探索}
\label{alg:pbnf}
\begin{algorithmic}[1]
\State $limit_f \leftarrow 0$
\While {true}
    \State $openList$を空のスタックとして初期化
    \State 初期ノード$v_0$を$openList$に追加
    \State $f_{next} \leftarrow \infty$
    \State $f_{next}$は次回の$limit_f$更新時に利用される 
    \While {$openList$が空ではない}
        \State $v_{cur}$ $\leftarrow$ $openList$から先頭ノードを取り出す
        \State $openList$から$v_{cur}$を削除
        \If {$v_{cur} \in 終了状態$}
            \State $v_0$から$v_{cur}$までの遷移と、移動コスト$g(v_{cur})$を解として出力して終了
        \EndIf
        \For{each $v_{next}$ $\in$ $children(v_{cur})$ do}
            \State $f_{new} \leftarrow g(v_{cur}) + cost(v_{cur}, v_{next}) + h(v_{next})$
            \If {$f_{new}(v_{next}) \leqq limit_f$}
                \State $openList$に$v_{next}$を追加
            \Else
                \State $f_{next} \leftarrow min(f_{next}, f_{new})$
            \EndIf
        \EndFor
    \EndWhile
    \State $update(limit_f)$
\EndWhile
\end{algorithmic}
\end{algorithm}
\newpage



% \chapter{24puzzle上でのBPIDA*探索について}
\chapter{GPUを使った並列IDA*探索における先行研究}
% \section{GPUを使った並列IDA*探索における先行研究}
堀江らの先行研究\cite{HA17}では、GPUを使ったスレッドベースの並列化を行うParallel IDA*探索とブロックベースの並列化を行うBlock Parallel IDA*探索が提唱された。
本節では、この二つ手法についてそれぞれ説明する。

% \subsection{Parallel IDA*探索}
\section{Parallel IDA*探索}
Parallel IDA*探索\cite{HA17}はGPUを利用したスレッドベースの並列IDA*探索である。この手法の大まかな流れは以下である。
\begin{enumerate}
 \item CPU側で根集合を作成する。
 \item GPU側でスレッドごとに根集合の各要素を初期状態としたf値制限深さ優先探索を並列に行う。この並列な探索の途中で適宜動的なロードバランスを行う。それぞれの探索の結果をCPU側に返す。
 \item 解が見つかった場合にはそれを答えとして出力しプログラムを終了する。解が見つからなかった場合、GPU側での各f値制限深さ優先探索における統計情報を元に、根集合の再構成を行う。これが静的なロードバランスである。f値の上限を更新し、2に戻る。
 \item 解が見つかるまで2-3の処理を繰り返す。
\end{enumerate}

GPUの性能をうまく引きだすには、各スレッドごとの仕事量の偏りを減らすことが必要である。Parallel IDA*探索では、静的なロードバランスと動的なロードバランスの二つを行う。

{\bf 静的なロードバランス}とは根集合をうまく生成することで、各スレッドの仕事量の偏りを減らす処理である。

{\bf 動的なロードバランス}とはスレッドごとのf値制限深さ優先探索を行っている際中に、残っている仕事量が均等になるよう再分配を行う処理である。

先行研究\cite{HA17}におけるGPU上のParallel IDA*探索のソルバは、CPU上の１スレッド直列実行時のソルバより低速であり、十分な高速化を行うことができなかった。これはGPUの同一のブロック内のスレッドがそれぞれ独立したf値制限深さ優先探索を行うため、ワープダイバージェンスが大きく、GPUの性能をうまく引き出すことができなかったからである。そこで、同一ブロック内のスレッドの仕事量の偏りを避け、ワープダイバージェンスを抑える手法として、Block Parallel IDA*(BPIDA*)探索\cite{HA17}が提案された。次の節ではこのBPIDA*探索について紹介する。

\newpage
\begin{algorithm}[H]
\caption{Parallel IDA*探索}
\label{alg:pbnf}
\begin{algorithmic}[1]
\Function{DFS}{$root, goals, limit_f$}
    \State $openList$を空のスタックとして初期化
    \State 初期ノード$root$を$openList$に追加
    \State $f_{next} \leftarrow \infty$
    \State $f_{next}$は次回の$limit_f$更新時に利用される
    \State ロードバランスを行うために探索の統計情報を$stat$として保持
    \While {$openList$が空ではない}
        \State $v_{cur}$ $\leftarrow$ ${ParallelPop}(openList)$
        \If {$v_{cur} \in goals$}
            \State $root$から$v_{cur}$までの遷移と、移動コスト$g(v_{cur})$を解として出力して終了
        \EndIf
        \If {$dynamic loab balance is triggered$}
            \State $DynamicLoadBalance(stat)$
        \EndIf
        \For{each $v_{next}$ $\in$ $children(v_{cur})$ do}
            \State $f_{new} \leftarrow g(v_{cur}) + cost(v_{cur}, v_{next}) + h(v_{next})$
            \If {$f_{new}(v_{next}) \leqq limit_f$}
                \State $openList$に$v_{next}$を追加
            \Else
                \State $f_{next} \leftarrow min(f_{next}, f_{new})$
            \EndIf
        \EndFor
    \EndWhile
    \State \Return $f_next, stat$
\EndFunction
\Function{Parallel IDA*}{$start, goals$}
    \State $rootSet \gets {CreateRootSet}(start, goals)$
    \State $limit_f \leftarrow {DecideFirstLimit}(rootSet)$
    \While {最短距離が発見されるまで}
        \ParallelForByThreads{$each root \in rootSet do$}
            \State $limit_f, stat \gets {DFS}(root, goals, limit_f)$
        \EndParallelForByThreads
        \State $UpdateRootSet(rootSet, stat)$
    \EndWhile
\EndFunction

\end{algorithmic}
\end{algorithm}
\newpage


% \subsection{Block Parallel IDA*探索}
\section{Block Parallel IDA*探索}
GPUを使用したIDA*探索の並列化の一つにBlock-Parallel IDA*(BPIDA*)探索\cite{HA17}がある。
前節の並列化手法では、一つのスレッドに対して一つの頂点を割り当て、その頂点を根とするf値制限深さ優先探索を並列に行い、処理を分割した。それに対し、BPIDA*探索では、一つのブロックに対して一つの頂点を割り当て、その頂点を根とするf値制限深さ優先探索を行う。
この手法はオセロのゲーム木探索において、64個の着手可能性を同一ブロック内のそれぞれのスレッドで判定し、盤面の評価を行うRockiらの研究\cite{RS09}から着想を得たものである。
各スレッドが独立したスタックを持ちf値制限深さ優先探索を行う場合より、BPIDA*探索では単純な処理の繰り返しとなるため、ワープダイバージェンスが発生しにくい。またブロック間のスレッドが一つのスタックを共有するため、動的に同一ブロック内のスレッドで仕事量の偏りを減らすことが可能である。このため、BPIDA*探索では明示的には、動的なロードバランスを行わない。

これによって先行研究\cite{HA17}では、GPU上のBPIDA*探索のソルバは、CPU上の直列実行のIDA*探索のソルバより6.78倍の高速化に成功した。

Algorithm 4はBPIDA*探索の疑似コードである。このアルゴリズムは一部の操作をのぞいて、前節のParallel IDA*探索と類似している。以下、アルゴリズムの詳細について説明する。

同一ブロック内の全てのスレッドが共有する一つのスタックが$sharedOpenList$である。このスタックには二つの並列な操作、$parallelPop$と$atomicPut$を行う。$parallelPop$では、$sharedOpenList$から1ブロック内のスレッド数を行為の数で割った数だけのノードを一度に取り出す。各スレッドでは取り出されたノードから、それぞれ対応する一つの遷移状態の遷移可能性の判定を行い、遷移可能であれば、処理$atomicPut$を行う。
$atomicPut$では$v_{next}$を$sharedOpenList$に並列に挿入する。この操作はある順序で直列に実行された場合と同様の振る舞いをする。

静的なロードバランスについては、6-18行目の繰り返しの回数を仕事量の見積もりとみなすことでロードバランスを行う。
あるノード$n$をあるブロックに一つ割り当て、f値制限深さ優先探索を行った際の繰り返しの回数を$load(n)$とする。ある根集合における$load(n)$の合計を$totalLoad$、根集合のノードの数を$nodeNum$としたとき$avarageLoad = totalLoad / nodeNum$とする。このとき、$load(n) / m \leqq averageLoad$を満たすような正の整数$m$の数だけ、あるノード$n$を分割する。ここでいう分割とは、根集合からノード$n$を取り出し、ノード$n$を初期状態として、$openList$のノードの数が$m$以上となるまでA*探索を行い、その際の$openList$のノードを根集合に再び追加する作業のことである。またこの作業の途中で、解が見つかった場合、最適解ではない可能性があるため、展開せずに再び$openList$に戻す必要があることに注意しなければならない。
この作業によって、探索が進むにつれ、仕事量の多いノードは展開されより多くのブロックに処理が分割され、静的なロードバランスを行うことができる。

\newpage
\begin{algorithm}[H]
\caption{Block Parallel IDA*探索}
\label{alg:pbnf}
\begin{algorithmic}[1]
\Function{BPDFS}{$root, goals, limit_f$}
    \State $sharedOpenList$を空のスタックとして初期化し共有メモリにおく
    \State 初期ノード$root$を$sharedOpenList$に追加
    \State $f_{next} \leftarrow \infty$
    \State $f_{next}$は次回の$limit_f$更新時に利用される 
    \While {$sharedOpenList$が空ではない}
        \State $v_{cur}$ $\leftarrow$ ${parallelPop}(sharedOpenList)$
        \If {$v_{cur} \in goals$}
            \State $root$から$v_{cur}$までの遷移と、移動コスト$g(v_{cur})$を解として出力して終了
        \EndIf
        \State $v_{next} \gets $$v_{cur}$を($threadId$を行為数で割った余り)番目の行為で遷移した時の状態
        \State $f_{new} \leftarrow g(v_{cur}) + cost(v_{cur}, v_{next}) + h(v_{next})$
        \If {$f_{new}(v_{next}) \leqq limit_f$}
            \State ${AtomicPut}(sharedOpenList, v_{next})$ \Comment{sharedOpenListに$v_{next}$に追加}
        \Else
            \State $f_{next} \leftarrow min(f_{next}, f_{new})$
        \EndIf
    \EndWhile
    \State \Return $f_next, stat$
\EndFunction
\Function{BPIDA*}{$start, goals$}
    \State $rootSet \gets {CreateRootSet}(start, goals)$
    \State $limit_f \leftarrow {DecideFirstLimit}(rootSet)$
    \While {最短距離が発見されるまで}
        \ParallelForByBlocks{$each root \in rootSet do$}
            \State $limit_f, stat \gets {BPDFS}(root, goals, limit_f)$
        \EndParallelForByBlocks
        \State $UpdateRootSet(rootSet, stat)$
    \EndWhile
\EndFunction

\end{algorithmic}
\end{algorithm}
\newpage

\chapter{24puzzle上でのBPIDA*探索について}
% \section{予備実験: 15-puzzleを用いたIDA*探索、およびBPIDA*探索の性能評価}
% 先行研究におけるBPIDA*探索\cite{HA17}で、性能評価に使用された15-puzzleの問題集と同じ問題集を使用して、自身
\section{実験1: 24-puzzleを用いたIDA*探索、およびBPIDA*探索の性能評価}
先行研究におけるBPIDA*探索\cite{HA17}では、15-puzzleを用いて、その性能を評価した。BPIDA*探索ではブロック間で共有するスタックを64Kbyteの共有メモリ上におく。堀江らの研究では、15-puzzleを解くのに必要なスタックのメモリの大きさは3Kbyte程度で済んだ。
必要なスタックのサイズは状態表現や、解の深さに比例し、問題によってはより大きなメモリを必要とする。

本章では、探索空間が15-puzzle問題のおよそ$10^{12}$倍ほど大きい24-puzzle問題を使い、より大きな共有メモリが必要な問題に対して、BPIDA*探索がどのような性能を示すか調査を行った。


\subsection{sliding puzzle}
sliding puzzleとはケースの中に収められたブロックをスペースに隣接するブロックを動かすことで、任意の配置から目的の配置を目指すパズルである。特に$k * k$の正方形の中に、$ k * k - 1$枚のブロックと一つのスペースが存在するsliding puzzleをN-puzzleと呼ぶ。可能な遷移の数はスペースの配置によって決まり、$2\sim4$である。
N-puzzle問題は、Nが増えるにつれてその探索空間は大きくなり、$N=15$のときで、盤面の数は$10^{13}$、$N=25$では、盤面の数は$10^{25}$である。
またヒューリスティック関数として、マンハッタン距離やPattern Databseなどがあり、問題の性質などが十分研究されている。

先行研究におけるBPIDA*探索\cite{HA17}では$N=15$の時における15-puzzle問題が評価の対象として使用されていた。
本研究では、探索空間が15-puzzle問題のおよそ$10^{12}$倍大きく、より大きなスタックをメモリ領域に必要とする24-puzzleを性能評価に使用する。
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=5cm, bb=0 0 300 400]{24puzzle.png}
\caption{24-puzzle}
\end{center}
\end{figure}


\subsection{実験の設定}
本実験では、50問の24-puzzleの問題を生成し、これを評価の対象として使用した。問題はおおむね難易度順に並んでおり、後半の問題ほど最適解までの深さが大きい。問題集の詳細については本論文の付録に追加する。マンハッタン距離をヒューリスティック関数に使用した、IDA*探索、およびBPIDA*探索\cite{HA17}のアルゴリズムを使用し、問題集を解くのに必要な実行時間がどれほどであるかを検証する。またそれぞれのソルバをソルバIDA*CPU、ソルバBPIDA*と呼ぶ。これらのソルバは最適解を全て探索するため、同じヒューリスティック関数を使用し同じ問題を解いた場合、展開順序にかかわらず、その時の展開ノードの数は等しくなる。

また実験ではAWSのGPUインスタンスであるg2.2xlargeインスタンスを使用した。CPUはIntel Xeon E5-2670であり、GPU
は NVIDIA GRID K520を搭載している。GPU環境の要約は以下のとおりである。
\begin{itemize}
 \item global RAM: 4GiB
 \item shared RAM/block: 48KiB
 \item CUDA cores: 1536
 \item warp size: 32
 \item GPU clock rate: 0.80GHz
\end{itemize}

\subsection{評価}
図3.2は50問の問題集を順に解いた際の、問題ごとのソルバIDA*CPUとソルバBPIDA*の実行時間を表したものである。縦軸は秒単位の実行時間、横軸は問題の番号である。

50問の問題集を全て解くのにソルバIDA*CPUでは6050.81秒、ソルバBPIDA*では4854.75秒ほどかかった。これはCPU上での直列実行であるソルバIDA*CPUと比較してソルバBPIDA*は1.25倍高速である。


\begin{figure}[H]
\begin{center}
\includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.5]{result1.png}
\caption{IDA*CPUとBPIDA*の実行時間の比較}
\end{center}
\end{figure}

\begin{table}[H]
\centering
\caption{Total Runtimes for 50 24-Puzzle Instances}
\label{my-label}
\begin{tabular}{|l|c|}
\hline
configuration & \multicolumn{1}{l|}{total runtime(seconds)} \\ \hline
\multicolumn{2}{|l|}{CPU-based sequential algorithms} \\ \hline
IDA*CPU & 6050.81 \\ \hline
\multicolumn{2}{|l|}{GPU-based parallel algorithms} \\ \hline
BPIDA* & 4854.75 \\ \hline
\end{tabular}
\end{table}


\subsection{考察}
15-puzzleを用いた堀江らの研究\cite{HA17}では、BPIDA*探索はCPU上でのIDA*探索のおよそ6.78倍の高速化を示した。

それに対し、24-puzzleを用いた今回の実験では、CPU上の直列IDA*探索がBPIDA*探索では1.25倍の高速化であり、十分な高速化を行うことができなかった。

なお本実験で用いたBPIDA*のコードをベースにした15-puzzleでは、堀江らの研究と同程度の高速化を得ることができていた。同程度の高速化を行っていた15-puzzleでのBPIDA*探索では、堀江らの実装とほぼ同じ3Kbyteほどの大きさのスタックを共有メモリにおいて使用していた。

今回の24-puzzleの問題では、24-puzzleを解く上でノードの保持する盤面の状態の数が16から25に増えた。また問題集の後半の一部問題で、スタックの保持する状態の数が、15-puzzleでの実装では不足し、エラーを起こすことがあったため、スタックの保持できる状態数を増やした。
これにより、共有メモリにおけるスタックのサイズは18Kbyteほどになっていた。

共有メモリはそのメモリ空間をL1キャッシュと共有する。L1キャッシュはグローバルメモリやレジスタピルによるアクセスの遅延を隠蔽する。BPIDA*探索では、カーネル実行の際、現在のノードから次の遷移状態を作成し、それらが遷移可能かどうかの判定、ヒューリスティック関数によるf値の計算などを行うために、多くの自動変数を使用する。これらはレジスタに登録され、レジスタが足りなくなった場合、ローカルメモリに保存される。ローカルメモリからのロードは、L1キャッシュに保存されることがあるため、レジスタを多く必要とするようなプログラムでは、L1キャッシュが性能向上に不可欠である。
今回の結果は、共有メモリの領域が大きくなったことで、使用できるL1キャッシュの領域が減り、これが探索に悪影響を与えているのではと予想した。

次章では、スタックを共有メモリではなく、グローバルメモリにおいたBPIDA*GLOBALを提案すると同時に、共有メモリとL1キャッシュの割合が探索にどのような影響を与えるか、検証を行う。


% \subsection{比較対象となる実装}
% Horieらの研究\cite{HA17}では、Burnsらの研究\cite{BHLR12}をもとに15-puzzleのCPUを用いた直列IDA*探索の実装を行っている。このソルバは、様々な最適化を施したBurnsらの直列IDA*探索の実装よりも1.31倍ほど高速なソルバであり、現在入手可能な実装のうち、最も効率的な直列IDA*探索の一つである。なおこのソルバはグラフ探索問題において最短経路となる解を一つのみ発見するソルバである。このソルバで採用されている最適化の一つに、問題集に対して最も実行時間の早くなる順に頂点を展開するというものがある。しかしながらこれは、本来事前にはわからない問題集の情報をもとによる最適化である。よってこのソルバを最短経路となる解を全て発見するソルバに拡張を行った。このソルバをソルバHとし、以降、本研究で比較対象となるソルバは全て、最短経路となる解を全て発見するグラフ探索問題に拡張をしたものとする。

% またHorieらの研究で実装されたBPIDA*探索のソルバをソルバBPIDA*(origin)とする。

% 自身の研究のベースラインとして、IDA*CPU探索のソルバ、Horieらの研究\cite{HA17}で提案されたPsimpleおよび、BPIDA*探索のソルバを作成した。Psimpleは並列IDA*探索の素朴な実装であり、プログラム開始時に根集合を一度だけ作成し、各スレッドに根集合から一つずつ根を割り当ててf値制限深さ優先探索を行う。このとき静的なロードバーランスや動的なロードバランスを何も行わないといったものである。


\chapter{Globalメモリを使用した並列IDA*探索と共有メモリ}

\section{BPIDA*GLOBAL探索}
本節では、BPIDA*探索\cite{HA17}に変更を加えた、BPIDA*GLOBAL探索を提案する。

従来のBPIDA*探索では、各ブロックごとにf値制限深さ優先探索を行う際、使用するスタックを共有メモリにおいた。BPIDA*GLOBAL探索では、BPIDA*探索で使用するスタックを共有メモリではなく、グローバルメモリにおくことを提案する。
このアルゴリズムにはBPIDA*探索と比較した際、二つのメリットと一つのデメリットが考えられる。

一つ目のメリットは最適解までの深さや状態表現が大きく、メモリに大きなスタックを必要とする問題であっても、共有メモリの容量を小さく保てることである。これは、共有メモリとメモリ領域を共有するL1キャッシュに大きな領域を割り当てれることを意味する。L1キャッシュはグローバルメモリへのアクセスや、レジスタピルによる遅延を隠蔽するため、探索効率が向上することが考えられる。

二つ目は、スタックが共有メモリに収まらない問題を解くことが可能になることである。共有メモリの容量である64Kbyteは決して大きいとは言えず、問題の種類によっては、スタックが足りなくなることが考えられる。グローバルメモリはCUDAにおいて最もメモリ領域の大きなメモリであり、より大きな領域をスタックとして割り当てることが可能である。

デメリットとしては、共有メモリからグローバルメモリへアクセスしなければならなくなったことによる遅延である。グローバルメモリは容量は大きいものの、CPU側に存在するオフチップメモリであり、GPU側に存在するオンチップメモリである共有メモリよりも、メモリへの読み書きが低速である。

次節では、このBPIDA*GLOBAL探索が実際にどのような性能を示すか実験を行う。

\newpage
\begin{algorithm}
\caption{Block Parallel IDA* Global}
\label{alg:pbnf}
\begin{algorithmic}[1]
\Function{BPDFS}{$root, goals, limit_f$}
    \State 初期ノード$root$を$globalOpenList$に追加
    \State $f_{next} \leftarrow \infty$
    \State $f_{next}$は次回の$limit_f$更新時に利用される 
    \While {$globalOpenList$が空ではない}
        \State $v_{cur}$ $\leftarrow$ ${ParallelPop}(globalOpenList)$
        \If {$v_{cur} \in goals$}
            \State $root$から$v_{cur}$までの遷移と、移動コスト$g(v_{cur})$を解として出力して終了
        \EndIf
        \State $v_{next} \gets $$v_{cur}$を($threadId$を行為数で割った余り)番目の行為で遷移した時の状態
        \State $f_{new} \leftarrow g(v_{cur}) + cost(v_{cur}, v_{next}) + h(v_{next})$
        \If {$f_{new}(v_{next}) \leqq limit_f$}
            \State ${AtomicPut}(globalOpenList, v_{next})$ \Comment{sharedOpenListに$v_{next}$を原子性を保ちつつ追加}
        \Else
            \State $f_{next} \leftarrow min(f_{next}, f_{new})$
        \EndIf
    \EndWhile
    \State $update(limit_f)$

    \State \Return $a$
\EndFunction
\Function{BPIDA*}{$start, goals$}
    \State $rootSet \gets {CreateRootSet}(start, goals)$
    \State $limit_f \leftarrow {DecideFirstLimit}(rootSet)$
    \State それぞれのブロックごとに使う$globalOpenList$を空のスタックとして初期化
    \While {最短距離が発見されるまで}
        \ParallelForByBlocks{$each root \in rootSet do$}
            \State $limit_f, stat \gets {BPDFS}(root, goals, limit_f)$
        \EndParallelForByBlocks
        \State $UpdateRootSet(rootSet, stat)$
    \EndWhile
\EndFunction

\end{algorithmic}
\end{algorithm}
\newpage

\section{実験2: 24-puzzleを用いたBPIDA*GLOBAL探索の性能評価}
\subsection{実験の設定}
本実験では実験1と同じ24-puzzleの自作した50問の問題集をベンチマークとして使用し、BPIDA*GLOBAL探索における実行時間の計測を行った。またこのBPIDA*GLOBAL探索のソルバをソルバBPIDA*GLOBALとした。これらのソルバは最適解を全て探索するため、同じヒューリスティック関数を使用し同じ問題を解いた場合、展開順序にかかわらず、その時の展開ノードの数は等しくなる。

また実験に使用したGPUインスタンスの環境は実験1と同じものである。


\subsection{評価}
図4.1は実験1の結果にランダムに自作した50問の問題集を順に解いた際の、ソルバBPIDA*GLOBALの実行時間の結果を追加したものである。縦軸は秒単位の実行時間、横軸は問題の番号である。

% ソルバIDA*CPUとソルバBPIDA*の実行時間を表したものである。
50問の問題集を全て解くのにソルバBPIDA*GLOBALでは1321.75秒かかった。これはソルバIDA*CPUの4.58倍、ソルバBPIDA*の1.25倍高速である。

またBPIDA*GLOBALでの共有メモリの使用量は4byteであった。これはグローバルメモリにおいたスタックにアクセスする際に、スタックのインデックスを共有メモリにおいたためである。
これに対して、BPIDA*では固定長の配列をスタックとして共有メモリに置き、その用量は17636byteであった。

\begin{figure}[H]
\begin{center}
\includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.5]{result2.png}
\caption{IDA*CPU、BPIDA*、BPIDA*GLOBALの実行時間の比較}
\end{center}
\end{figure}

\begin{table}[H]
\centering
\caption{Total Runtimes for 50 24-Puzzle Instances}
\label{my-label}
\begin{tabular}{|l|c|}
\hline
configuration & \multicolumn{1}{l|}{total runtime(seconds)} \\ \hline
\multicolumn{2}{|l|}{CPU-based sequential algorithms} \\ \hline
IDA*CPU & 6050.81 \\ \hline
\multicolumn{2}{|l|}{GPU-based parallel algorithms} \\ \hline
BPIDA* & 4854.75 \\
BPIDA*GLOBAL & 1321.75 \\  \hline
\end{tabular}
\end{table}


\subsection{考察}
BPIDA*で17636byteあった共有メモリの領域がBPIDA*GLOBALでは、4byteに減った。これは、共有メモリが減ったことで、共有メモリと64Kbyteの領域を共有するL1キャッシュの領域が増えたことを意味する。L1キャッシュは、グローバルメモリへのアクセスやレジスタピルによる遅延を隠蔽できる。そのため、共有メモリの領域が減ったことが、BPIDA*GLOBALの探索性能の向上に寄与したのではと考えられる。

次に、ソルバBPIDA*では共有メモリに存在していたスタックをソルバBPIDA*GLOBALでは、グローバルメモリへ移動させた。グローバルメモリへのアクセスは400～600クロックサイクルほどの遅延を含み、共有メモリと比べて低速である。当然スタックを共有メモリからグローバルメモリへ移動させれば、スタックへの読み書きの遅延は大きくなる。これはBPIDA*GLOBALの探索性能の低下に寄与すると考えられる。

ソルバBPIDA*GLOBALがソルバBPIDA*の3.67倍高速であったことを考えると、共有メモリの占有領域を減らしL1キャッシュの領域を増やしたことによる探索性能向上の効果が、グローバルメモリにアクセスすることへの性能劣化より大きかったと言える。



% また今回の問題では、17636byteのメモリ容量のスタックで全ての問題を解くことができたが、


% 堀江らの研究\cite{HA17}では、24-puzzleより少ないメモリのスタックで探索が可能な、15-puzzleにおいて、BPIDA*探索はCPU上でのIDA*探索のおよそ6.78倍の高速化を示した。グローバルメモリへのアクセスによる遅延がある分、メモリ容量の少ない

% より状態表現の大きい問題、最適解への距離が深い問題、す
\section{実験3: 共有メモリとL1キャッシュの領域が並列IDA*探索に与える影響の評価}


\subsection{実験の設定}
本実験では実験1と同じ24-puzzleの自作した50問の問題集をベンチマークとして使用し、BPIDA*GLOBAL探索において異なる大きさのメモリを共有メモリに配置した時の実行時間を計測した。
この領域の大きさはそれぞれ、4412byte、8820byte、13228byte、17636byte、35268byteであり、それぞれのソルバを、ソルバBPIDA*GLOBAL(4KB)、ソルバBPIDA*GLOBAL(9KB)、ソルバBPIDA*GLOBAL(13KB)、ソルバBPIDA*GLOBAL(18KB)、ソルバBPIDA*GLOBAL(35KB)とよぶ。またこれらの配置した配列は、領域を確保しただけであり、アルゴリズムの探索挙動に影響を与えない。

これにより、共有メモリの占有領域がGPU上での並列IDA*探索の探索効率にどのような影響を与えるのか、より詳細な知見を得ることを目指す。

また実験に使用したGPUインスタンスの環境は実験1と同じものを使用した。


\subsection{評価}
図4.2は、前回までの実験で使用したものと同様の50問の問題集に対して、異なる大きさの領域を共有メモリに確保した際のBPIDA*GLOBALの実行時間である。縦軸は秒単位の実行時間、横軸は問題の番号である。

50問の問題集を全て解くのにソルバBPIDA*GLOBAL(4KB)で1822.08秒、ソルバBPIDA*GLOBAL(9KB)で3345.17秒、ソルバBPIDA*GLOBAL(13KB)で5401.92秒、ソルバBPIDA*GLOBAL(18KB)で8066.62秒、ソルバBPIDA*GLOBAL(35KB)で16104.5秒であった。
共有メモリに確保した領域が大きくなるにつれて探索に必要な時間は大きくなっていることがわかる。

またソルバBPIDA*GLOBAL(18KB)の使用した共有メモリの大きさ17636byteは前章の実験で使用したソルバBPIDA*における共有メモリのサイズと同じである。ソルバBPIDA*GLOBAL(18KB)はソルバBPIDA*の1.66倍ほど低速である。


ソルバBPIDA*GLOBAL(4KB)、ソルバBPIDA*GLOBAL(9KB)はソルバBPIDA*が50問の問題集をすべて解くのに必要とした4854.75秒より高速であり、ソルバBPIDA*GLOBAL(13KB)、ソルバBPIDA*GLOBAL(18KB)、ソルバBPIDA*GLOBAL(35KB)はそれより低速である。

\begin{figure}[H]
\begin{center}
\includegraphics[keepaspectratio, width=7cm, bb=0 0 624 466, scale=0.5]{result3.png}
\caption{共有メモリに異なるサイズの領域を確保した際のBPIDA*GLOBALの実行時間の比較}
\end{center}
\end{figure}

\begin{table}[H]
\centering
\caption{Total Runtimes for 50 24-Puzzle Instances}
\label{my-label}
\begin{tabular}{|l|c|}
\hline
configuration & \multicolumn{1}{l|}{total runtime(seconds)} \\ \hline
\multicolumn{2}{|l|}{GPU-based parallel algorithms} \\ \hline
BPIDA*GLOBAL    & 1321.75  \\
BPIDA*GLOBAL (4412byte) & 1822.08  \\
BPIDA*GLOBAL (8820byte) &  3345.17 \\
BPIDA*GLOBAL (13228byte) & 5401.92 \\
BPIDA*GLOBAL (17636byte) & 8066.62 \\
BPIDA*GLOBAL (35268byte) & 16104.5 \\ \hline
\end{tabular}
\end{table}

\subsection{考察}
同じ領域の共有メモリが使用された前回までの実験におけるソルバBPIDA*とソルバBPIDA*GLOBAL(18Kbyte)を比較すると、1.66倍ソルバBPIDA*の方が高速であった。これらの遅延には2つの要因があると推測できる。

一つは、BPIDA*GLOBALではスタックを共有メモリから、グローバルメモリにおいたことで、スタックそのものへのメモリアクセスが遅くなったことである。

二つ目は、スタックがグローバルメモリへ置かれたことで、より多くのL１キャッシュを必要としたことである。L１キャッシュはグローバルメモリへのアクセスの遅延の隠蔽を行う。BPIDA*GLOBALでは当然グローバルメモリへのアクセスが、BPIDA*よりスタックへのアクセスによって多くなる。このためより多くのL１キャッシュが必要になったと思われる。

これらが、遅延の理由として考えられる。また同時にこの二つの遅延の効果が今回の実験において、1.66倍程度であるということも言える。そのため、それよりも大きな高速化が得られる場合、これらの遅延を隠蔽できると推測できる。

ソルバBPIDA*GLOBAL(4Kbyte)、ソルバBPIDA*GLOBAL(9Kbyte)、ソルバBPIDA*GLOBAL(13Kbyte)、ソルバBPIDA*GLOBAL(18Kbyte)、ソルバBPIDA*GLOBAL(35Kbyte)を比較すると、L1キャッシュの領域が減ることで探索効率が著しく劣化することが確認できる。

ソルバBPIDA*と同程度の共有メモリを使用するソルバBPIDA*GLOBAL(18Kbyte)が、ソルバBPIDA*GLOBALより6.10倍遅くなっていることを見ると、3章においてBPIDA*が十分な高速化を示すことができなかったことの原因として共有メモリに置くスタックが増えたことが大きな原因であると言える。

そのため、状態表現が大きい、最適解までの深さが大きく、共有メモリに大きなスタックを必要とするような問題集の一つで、グローバルメモリにスタックを置くBPIDA*GLOBAL探索が有効な手法であったことを示せた。





\chapter{PDBを使用した並列IDA*探索}
\section{patttern database}
pattern database(PDB)\cite{CS98}とはN-puzzle問題を解く際のヒューリスティック関数の一つである。
PDBでは、事前にN-puzzleをいくつかの部分問題として分割し、どのグループにも属さないタイルは空きマスと見なした場合に、各グループに属するタイルが終了状態の位置まで移動するのに要する最短コストを計算した表を作成する。
各グループにおけるゴール状態までの最短コストの和をとることで、これが実際の移動コストの下界となる。
これは、グループに属さないタイルを無視した際に、各グループに属するタイルが正しい位置へ移動するまでの最短コストであるため、実際より少ない移動コストで終了状態にたどり着くことが不可能だからである。よってPDBはadmissibleなヒューリスティック値を示す。

また各グループを1枚ずつのタイルとして分ける場合、これはマンハッタン距離を用いたヒューリスティックと同じとなる。より大きな状態空間を事前計算しておけば、PDBを用いたヒューリスティック関数の精度は上がる一方、必要となるメモリの量は増える。

% \begin{figure}[H]
% \begin{center}
% \includegraphics[width=5cm, bb=0 0 175 89]{pdb.pdf}
% \caption{24-puzzleにおけるPDBの分割(\cite{PDB}より引用)}
% \end{center}
% \end{figure}


\begin{figure}[htbp]
\begin{center}
\includegraphics[width=5cm, bb=0 0 610 445]{pdb.png}
\caption{24-puzzleにおけるPDBの分割}
\end{center}
\end{figure}

% それぞれの部分問題における最適解までの距離を事前に計算した表を用いて、
本論文ではKorfらの研究\cite{KF02}で提案された24-puzzleにおけるPDBの分割方法を紹介する。
Korfらの研究\cite{KF02}では図5.1の2種類の分割方法が提案された。この分割方法では各グループ6枚のタイルで4つのグループに24-puzzleを分割する。4つのグループのうち、3つは$2*3$の同じ形状の分割をしている。
同じ形状の分割では、1つの表を使い回すことでヒューリスティック値を計算することが可能である。そのため、実質2つの形状の表を使用することで、それぞれの部分問題におけるヒューリスティック値を計算することが可能であり、必要なメモリを減らすことができる。

また図5.1では2種類の分割方法であるが、これは片方の分割方法が、もう片方の分割方法の反射となっているため、同じ表を使うことでヒューリスティック値の計算が可能である。
本論文におけるPDBの実装では、この2種類の分割方法で計算したヒューリスティック値のうち大きな方を採用することで、f値を計算した。




\section{実験4: PDBを使用した並列IDA*探索の評価}
\subsection{実験の設定}
実験1と同じ問題集、同じGPU環境を使用し、ヒューリスティック関数としてPDBを使用した際のBPIDA*探索、BPIDA*Global、CPU上でのIDA*探索の性能評価を行った。
またそれぞれのソルバをソルバBPIDA*(PDB)、ソルバBPIDA*GLOBAL(PDB)、ソルバIDA*CPU(PDB)と呼ぶ。
これらのソルバは最適解を全て探索するため、同じヒューリスティック関数を使用し同じ問題を解いた場合、展開順序にかかわらず、その時の展開ノードの数は等しくなる。

\subsection{評価}
図5.2はソルバBPIDA*(PDB)、ソルバBPIDA*GLOBAL(PDB)、ソルバIDA*CPU(PDB)の3つのソルバにおける問題ごとの実行時間のグラフである。縦軸は秒単位の実行時間、横軸は問題の番号を表す。

50問の問題集を全て解くのにソルバIDA*CPU(PDB)では34.1312秒、ソルバBPIDA*(PDB)では16.2875秒、ソルバBPIDA*GLOBAL(PDB)では6.45871秒かかった。

直列な実装であるソルバIDA*CPU(PDB)とそれぞれの並列アルゴリズムを比較すると、ソルバBPIDA*(PDB)が2.10倍、
ソルバBPIDA*GLOBAL(PDB)が5.28倍高速である。

\begin{figure}[H]
\begin{center}
\includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.5]{result4.png}
\caption{ヒューリスティック関数にPDBを使用したIDA*CPU、BPIDA*、BPIDA*GLOBALの問題ごとの実行時間}
\end{center}
\end{figure}

\begin{table}[H]
\centering
\caption{Total Runtimes for 50 24-Puzzle Instances}
\label{my-label}
\begin{tabular}{|l|c|}
\hline
configuration & \multicolumn{1}{l|}{total runtime(seconds)} \\ \hline
\multicolumn{2}{|l|}{CPU-based sequential algorithms} \\ \hline
IDA*CPU (manhattan distance) & 6050.81 \\ 
IDA*CPU (PDB) & 34.1312 \\ \hline
\multicolumn{2}{|l|}{GPU-based parallel algorithms} \\ \hline
BPIDA* (manhattan distance)& 4854.75 \\
BPIDA*GLOBAL (manhattan distance) & 1321.75 \\
BPIDA* (PDB)& 16.2875 \\
BPIDA*GLOBAL (PDB) & 6.45871 \\ \hline
\end{tabular}
\end{table}


% \subsection{ヒューリスティック関数として、マンハッタン距離使用時とPDB使用時の実行時間の比較}


% 図5.2はソルバBPIDA*(PDB)、ソルバBPIDA*GLOBAL(PDB)、ソルバIDA*CPU(PDB)の3つのソルバにおける問題ごとの実行時間のグラフである。縦軸は秒単位の実行時間、横軸は問題の番号を表す。

% 50問の問題集を全て解くのにソルバIDA*CPU(PDB)では34.1312秒、ソルバBPIDA*(PDB)では16.2875秒、ソルバBPIDA*GLOBAL(PDB)では6.45871秒かかった。

% 直列な実装であるソルバIDA*CPU(PDB)とそれぞれの並列アルゴリズムを比較すると、ソルバBPIDA*(PDB)がおよそ2.10倍、
% ソルバBPIDA*GLOBAL(PDB)がおよそ5.28倍高速である。


図5.3、図5.4、図5.5はそれぞれ前章までの実験で測定したマンハッタン距離を使用したソルバIDA*CPU、ソルバBPIDA*GLOBAL、ソルバIDA*CPUの問題単位の実行時間を、PDBを用いたソルバIDA*CPU(PDB)、ソルバBPIDA*GLOBAL(PDB)、ソルバIDA*CPU(PDB)の問題単位の実行時間で割った割合を示している。
これらをそれぞれcpuPDBspeedup、bpida*PDBspeedup、bpida*globalPDBspeedupと定義する。
この三つは、マンハッタン距離使用時と比べ、PDBがどれほど、それぞれのアルゴリズムにおいて高速化に貢献したかを示す指標となる。

図5.3のcpuPDBspeedupに注目すると、最も高速化した45番の問題では3997倍の高速化に成功している。また概ね問題集の難易度が高くなる後半の問題ほど、高速化の割合は大きくなっている。

図5.4のbpida*PDBspeedupに注目すると、最も高速化した問題は45番であり、4432倍の高速化である。直列実行時のcpuPDBspeedupと同様に、後半の難易度の高い問題で高速化の割合が大きくなっている。

図5.5のbpida*globalPDBspeedupに注目すると、他のソルバと同様に最も高速化した問題は45番であり、2797倍である。前半の$0\sim30$番の問題では$1.52\sim168.27$倍の高速化であるが、後半の$40\sim49$番の問題では、$105.82\sim2855$倍の高速化に成功している。


図5.6は各問題ごとに、bpida*PDBspeedupをcpuPDBspeedupで割った割合である。$bpida^*PDBspeedup/cpuPDBspeedup > 1$である問題は、gpu上のBPIDA*探索での高速化の割合がcpu上での高速化の割合より大きい問題であることを示し、そのような問題は50問中23問であった。逆に$bpida^*PDBspeedup/cpuPDBspeedup < 1$である問題は、cpu上での高速化の割合がgpu上のBPIDA*探索での高速化の割合より大きい問題であることを示し、そのような問題は50問中27問であった。
また難易度の高い25番以降の問題に注目すると、すべての問題において、$bpida^*PDBspeedup/cpuPDBspeedup$は$0.54\sim1.94$倍の範囲に収まっている。

図5.7は各問題ごとに、bpida*globalPDBspeedupをcpuPDBspeedupで割った割合である。$bpida^*globalPDBspeedup/cpuPDBspeedup > 1$である問題は、gpu上のBPIDA*GLOBAL探索での高速化の割合がcpu上での高速化の割合より大きい問題であることを示し、そのような問題は50問中31問であった。逆に$bpida^*globalPDBspeedup/cpuPDBspeedup < 1$である問題は、cpu上での高速化の割合がgpu上のBPIDA*GLOBAL探索での高速化の割合より大きい問題であることを示し、そのような問題は50問中19問であった。
また難易度の高い25番以降の問題に注目すると、すべての問題において、$bpida^*PDBspeedup/cpuPDBspeedup$は$0.77\sim3.19$倍の範囲に収まっている。

\begin{figure}[H]
\begin{center}
\includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.2]{cpuPDBspeedup.png}
\caption{cpuPDBspeedup}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.2]{bpidaPDBspeedup.png}
\caption{bpida*PDBspeedup}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.2]{bpidaglobalPDBspeedup.png}
\caption{bpida*globalPDBspeedup}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{minipage}{0.5\hsize}
\begin{center}
\includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.5]{ratio1.png}
\caption{cpuPDBspeedup/bpida*PDBspeedup}
\end{center}
\end{minipage}
\begin{minipage}{0.5\hsize}
\begin{center}
\includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.5]{ratio2.png}
\caption{cpuPDBspeedup/bpida*globalPDBspeedup}
\end{center}
\end{minipage}
\end{figure}



\subsection{考察}
すべてのソルバにおいて、ヒューリスティック関数をマンハッタン距離からPDBに変更したことで、大幅な高速化に成功した。これはPDBの使用で、より精度の高い枝刈りが可能となり、最適解を全て見つけるまでに展開すべきノードの数が大幅に減少したことが原因と考えられる。後半の難易度の高い問題ほど、高速化の割合が大きくなったのは、枝刈りを行うことのできるノードの数が、最適解までの距離に比例し増加したためと思われる。

GPUを用いた並列IDA*探索では、マンハッタン距離の計算結果のテーブルを、共有メモリにおき、ヒューリスティック関数として使用していた。
PDBを使用する際、255Mbyteの領域を確保する必要があるので、これは64Kbyteしかメモリ容量をもたない共有メモリに置くことは不可能である。そこでより大きな容量を持つグローバルメモリにPDBを置く。これによって共有メモリではなく、グローバルメモリにアクセスする分の遅延がGPUを用いた並列IDA*探索では生じると推測できる。この遅延はCPU上のソルバにはない遅延である。従って、bpida*PDBspeedupとcpuPDBspeedup、bpida*globalPDBspeedupとcpuPDBspeedupを比較すればこの遅延がどれほどの効果があるのか確かめられると考えた。

結果を見ると、難易度の低い前半の問題では、$2\sim10$倍ほどcpu上での高速化の方が大きい結果が得られたが、25番以降の難易度の高い問題では、$0.5\sim2$倍の範囲に収まった。
これは難易度のある程度高い問題では、グローバルメモリにアクセスする分の遅延は大きくても2倍程度であり、これらの遅延より探索自体が高速になるような手法を用いれば、高速化が可能であることを示唆している。マンハッタン距離ではなくPDBをヒューリスティックに用いた場合、Korfらの研究\cite{KF02}では2000倍ほどの高速化が可能であることが報告されており、これはグローバルメモリへのアクセスを遅延を隠蔽するには十分な高速化であると思われる。

少なくともGPUを使用した並列IDA*探索において、高速にアクセスが可能な共有メモリにおいたマンハッタン距離を使用するより、低速なグローバルメモリを使用するPDBをヒューリスティック関数として使用した場合、高速化を行うことが可能であることが本論文で示せた。

% \begin{figure}[H]
% \begin{center}
% \includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.5]{result4.png}
% \caption{ヒューリスティック関数にPDBを使用したIDA*CPU、BPIDA*、BPIDA*GLOBALの問題ごとの実行時間}
% \end{center}
% \end{figure}

% \begin{figure}[H]
% \begin{center}
% \includegraphics[keepaspectratio, width=5cm, bb=0 0 610 445, scale=0.2]{cpuPDBspeedup.png}
% \caption{cpuPDBspeedup}
% \end{center}
% \end{figure}

% \begin{figure}[H]
% \begin{center}
% \includegraphics[keepaspectratio, width=5cm, bb=0 0 610 445, scale=0.2]{bpidaPDBspeedup.png}
% \caption{bpida*PDBspeedup}
% \end{center}
% \end{figure}

% \begin{figure}[H]
% \begin{center}
% \includegraphics[keepaspectratio, width=5cm, bb=0 0 610 445, scale=0.2]{bpidaglobalPDBspeedup.png}
% \caption{bpida*globalPDBspeedup}
% \end{center}
% \end{figure}


% \begin{figure}[H]
% \begin{minipage}{0.5\hsize}
% \begin{center}
% \includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.5]{ratio1.png}
% \caption{cpuPDBspeedup/bpida*PDBspeedup}
% \end{center}
% \end{minipage}
% \begin{minipage}{0.5\hsize}
% \begin{center}
% \includegraphics[keepaspectratio, width=7cm, bb=0 0 610 445, scale=0.5]{ratio2.png}
% \caption{cpuPDBspeedup/bpida*globalPDBspeedup}
% \end{center}
% \end{minipage}
% \end{figure}


% \begin{table}[H]
% \centering
% \caption{Total Runtimes for 50 24-Puzzle Instances}
% \label{my-label}
% \begin{tabular}{|l|c|}
% \hline
% configuration & \multicolumn{1}{l|}{total runtime(seconds)} \\ \hline
% \multicolumn{2}{|l|}{CPU-based sequential algorithms} \\ \hline
% IDA*CPU (manhattan distance) & 6050.81 \\ 
% IDA*CPU (PDB) & 34.1312 \\ \hline
% \multicolumn{2}{|l|}{GPU-based parallel algorithms} \\ \hline
% BPIDA* (manhattan distance)& 4854.75 \\
% BPIDA*GLOBAL (manhattan distance) & 1321.75 \\
% BPIDA* (PDB)& 16.2875 \\
% BPIDA*GLOBAL (PDB) & 6.45871 \\ \hline
% \end{tabular}
% \end{table}



\chapter{終わりに}
\section{本研究の成果のまとめ}
本研究では、GPU上のIDA*探索の並列化手法の一つBPIDA*探索において、先行研究\cite{HA17}より探索空間の大きい24-puzzle問題においてその性能の評価を3章で行った。
共有メモリに大きなスタックを必要とするような問題において十分な探索性能をBPIDA*探索が示せないことを実証した。

4章では、そのような問題への対策案としてBPIDA*GLOBALを提案し、より最適解までの距離が深い問題、状態表現の大きな問題において、従来のBPIDA*探索より効率よく探索を行うことが示せた。またその探索向上に共有メモリの領域が大きく寄与することを、BPIDA*GLOBAL探索において共有メモリに異なる領域の配列を配置することによって確認を行った。これによってL1キャッシュが減ることによってGPU上での並列IDA*探索が性能を劣化させることを示した。

5章では、マンハッタン距離より精度は高いが、より多くのメモリを必要とするPDBをヒューリスティック関数に用いたBPIDA*探索の評価を行った。PDBは255MByteほどの大きな領域をグローバルメモリに必要とするが、マンハッタン距離を使用した時より高速な探索がPDBを用いた並列化手法であるBPIDA*探索、およびBPIDA*GLOBAL探索で可能であることを示せた。

\section{今後の課題}
今後の課題としては、以下が挙げられる。

\subsection{N-puzzle以外の状態空間の問題への応用}
堀江らの先行研究\cite{HA17}では、15-puzzleをbenchmarkとして使用していた。
本研究では、24-puzzleをbenchamarkとして扱った。15-puzzleと比べて、24-puzzleの方が、探索空間ははるかに大きい一方、両者とも可能な遷移の数は同じ$2\sim4$である。BPIDA*探索は一つのブロックの中のスレッドの数を遷移の数で割った数だけ、オープンリストからノードを取り出す。そのため、遷移の数が異なる状態空間においては異なる性質や困難を持つ可能性がある。
% 共有メモリにアルゴリズムの探索挙動に影響を与えない無駄なメモリを確保し


\subsection{マルチGPUによる大規模並列化}
本研究では一つのGPUにおけるBPIDA*探索のメモリの配置や、より大きなメモリを要するヒューリスティック関数を使用した際の探索性能についての調査を行った。現在このBPIDA*探索を複数のGPUをまたいだ上での手法は提案されていない。
マルチGPUによる探索が可能になれば、より多くのブロック、より多くのスレッドを使用することができる。
一方、マルチGPUでは、BPIDA*探索で行っていた根集合の分配など、GPUをまたいだ情報の共有によるオーバーヘッドが想定される。
マルチGPUの強みを生かしつつ、情報共有のオーバーヘッドの少ないアルゴリズムを提案できればさらなる高速化を目指すことが可能ではと考えられる。

\subsection{グローバルメモリを使用した並列IDA*における重複検出}
本研究では、共有メモリにのせることの不可能なヒューリスティック関数であるPDBをグローバルメモリにのせることで、探索性能を向上させることができた。
また同様に、15-puzzleよりも大きな領域を必要とする24-puzzleにおいて、共有メモリにおいたスタックをグローバルメモリに移すことで、探索性能の向上が可能であった。これらは、より良いヒューリスティック関数の使用や、l1キャッシュの領域を大きくすることによる高速化が、共有メモリではなく、グローバルメモリにアクセスすることによる遅延を隠蔽できることを示した。
これらの知見をもとにすると、グローバルメモリを利用した重複検出による高速化が、グローバルメモリへのアクセスによる遅延を隠蔽し、さらなる高速化を行えることが示唆される。


%-----------------------------参考文献記述エリア---------------------------%
\begin{thebibliography}{10}
  \bibitem{HA17} Satoru Horie and Alex Fukunaga. 2017. Block-Parallel IDA* for GPUs. In SOCS.
  \bibitem{BHLR12}Ethan Andrew Burns, Matthew Hatem, Michael J Leighton, and Wheeler Ruml. Implementing fast heuristic search code. In SOCS, 2012.
  \bibitem{Kor85}Korf, R. E. 1985. Depth-first iterativedeepening: An optimal admissible tree search. Artificial intelligence 27(1):97-109.
  \bibitem{KF02}Korf, R. E., and Felner, A. 2002. Disjoint pattern database heuristics. Artificial intelligence 134(1):9-22.
  \bibitem{HNR68}Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, Vol. 4, No. 2, pp. 100-107, 1968. 
  \bibitem{Rus92}Russell, S. 1992. Efficient memory-bounded search methods. In Neumann, B. Proceedings of the 10th European Conference on Artificial intelligence. Vienna, Austria: John Wiley \& Sons, New York, NY. pp. 1–5.
  \bibitem{CS98}Joseph C Culberson and Jonathan Schaeffer. Pattern databases. Computational Intelligence, Vol. 14, No. 3, pp. 318-334, 1998.
  \bibitem{PDB}Ariel Felner1 and Amir Adler Solving the 24 Puzzle with Instance Dependent Pattern Databases, International Symposium on Abstraction, Reformulation, and Approximation SARA 2005: Abstraction, Reformulation and Approximation pp 248-260
  \bibitem{Powley, Ferguson and Korf 1993]} Powley, C.; Ferguson, C.; and Korf, R. E. 1993. Depth-First heuristic search on a SIMD machine. Artificial Intelligence 60(2):199-242.
  \bibitem{RS09}Rocki, K., and Suda, R. 2009. Parallel minimax tree searching on GPU. In International Conference on Parallel Processing and Applied Mathematics, 449-456. Springer.



\end{thebibliography}

\appendix
\chapter{実験の詳細}
性能評価のために用いた自己生成した24-puzzleの問題集の詳細および、それを解いた各種アルゴリズムの実行時間を付録に添付する。

24-puzzleの問題集については、パズルの初期状態、最短経路、マンハッタン距離もしくはPDBをヒューリスティック関数に使用した際にIDA*探索が、全ての最短経路を見つけるのに必要な、展開ノードの数について表記する。

各種アルゴリズムの実行時間はこの問題集を本論文における各種ソルバを用いて実行した時の各問題ごとの実行時間(sec)についての表記である。


\begin{table}[]
\centering
\caption{24-Puzzle problems}
\label{my-label}
\scalebox{0.6}{
\begin{tabular}{|c|c|c|c|c|}
\hline
問題ID & 初期状態 & 最短経路　&　展開ノード数(マンハッタン距離)　& 展開ノード数(PDB) \\ \hline
0 & 11 7 5 2 3 1 22 16 8 4 6 12 19 14 9 10 15 18 23 13 20 17 21 0 24  & 41 & 41430 & 1452 \\
1 & 5 7 1 9 3 11 2 8 4 19 6 10 12 22 24 15 21 14 13 0 20 17 18 16 23  & 41 & 32881 & 3480 \\
2 & 5 13 10 7 2 11 1 8 4 3 6 16 22 12 9 0 21 17 19 14 15 20 23 18 24  & 39 & 3843 & 231 \\
3 & 10 12 5 14 3 2 13 0 6 4 1 11 17 18 8 20 7 16 23 9 21 15 22 24 19  & 43 & 1310 & 173 \\
4 & 1 7 3 8 12 2 10 6 14 4 17 15 16 11 13 5 21 18 19 9 0 20 22 23 24  & 40 & 17475 & 1486 \\
5 & 5 7 13 0 4 11 1 2 10 8 6 18 3 16 19 15 21 23 14 9 20 22 12 17 24  & 47 & 1097483 & 52328 \\
6 & 6 2 7 9 3 1 15 5 18 4 16 10 13 12 14 17 8 22 11 23 20 0 21 24 19  & 43 & 70290 & 9338 \\
7 & 10 5 11 13 2 0 12 6 1 9 15 7 8 14 3 22 17 21 18 4 20 16 23 24 19  & 49 & 839687 & 6579 \\
8 & 0 5 1 3 4 6 11 2 7 9 15 17 23 8 24 12 21 13 14 18 20 22 10 19 16  & 42 & 89979 & 5184 \\
9 & 5 17 1 3 4 6 15 2 7 8 16 12 0 9 23 11 10 14 21 19 20 22 18 13 24  & 44 & 721374 & 11615 \\
10 & 2 6 12 3 4 7 1 21 9 14 15 13 0 8 19 20 5 11 10 24 22 23 16 17 18  & 50 & 4974216 & 19657 \\
11 & 5 1 13 19 3 6 8 17 4 12 10 11 7 9 2 20 0 15 23 14 21 22 16 24 18  & 50 & 7096983 & 322762 \\
12 & 2 15 7 10 4 5 13 3 18 8 6 17 1 24 9 16 0 11 12 14 20 21 23 22 19  & 50 & 5231891 & 85885 \\
13 & 7 5 2 9 0 12 13 8 14 3 6 20 1 4 17 16 21 23 18 19 11 15 10 22 24  & 54 & 2150531 & 635 \\
14 & 1 6 13 2 0 5 10 16 4 3 11 22 15 9 7 12 17 23 19 8 20 21 18 24 14  & 50 & 4984421 & 46280 \\
15 & 5 3 8 4 9 10 2 0 6 18 15 1 11 21 14 12 23 13 7 24 16 20 22 17 19  & 51 & 12735903 & 43852 \\
16 & 1 11 12 17 3 7 2 22 8 4 6 16 18 13 9 5 20 21 19 0 10 15 24 14 23  & 53 & 13641435 & 171951 \\
17 & 7 5 11 8 4 0 3 1 9 14 10 20 6 12 2 16 21 22 24 13 15 23 19 18 17  & 53 & 9139906 & 8804 \\
18 & 5 11 1 2 3 8 0 16 13 4 6 17 22 7 14 10 15 19 24 23 20 12 21 18 9  & 50 & 9086810 & 130405 \\
19 & 0 5 6 8 9 15 10 1 13 2 11 21 7 4 12 16 22 3 19 14 20 18 17 23 24  & 50 & 11979692 & 82284 \\
20 & 2 6 3 13 8 1 10 15 12 4 11 7 17 22 9 16 5 14 23 18 20 21 19 24 0  & 52 & 23879598 & 59725 \\
21 & 2 6 3 13 12 17 9 4 16 14 1 7 5 8 18 10 11 21 23 19 15 20 22 24 0  & 56 & 20780967 & 175587 \\
22 & 5 1 2 4 9 15 11 18 3 14 20 0 7 8 12 21 10 16 24 13 17 6 22 19 23  & 49 & 17516193 & 189094 \\
23 & 6 5 3 9 12 15 2 13 14 8 11 7 1 18 4 17 10 23 22 19 20 21 0 16 24  & 50 & 20110450 & 183748 \\
24 & 3 0 13 7 8 9 1 15 4 12 2 5 6 19 14 10 16 11 18 17 20 21 22 23 24  & 51 & 22571797 & 10558 \\
25 & 1 3 0 16 8 5 10 12 9 4 15 6 18 2 14 17 11 13 7 21 20 22 23 24 19  & 50 & 20021855 & 869330 \\
26 & 15 2 0 6 8 1 18 10 9 4 5 7 3 24 14 16 11 21 13 23 20 22 12 17 19  & 54 & 27410064 & 115075 \\
27 & 6 3 4 9 8 1 15 7 0 13 5 10 17 2 12 22 11 23 24 14 21 20 16 19 18  & 54 & 69779690 & 131720 \\
28 & 5 11 3 4 9 1 6 7 18 17 2 15 22 8 14 20 10 16 13 12 21 23 0 19 24  & 54 & 206588247 & 1548111 \\
29 & 5 6 14 8 13 11 1 2 22 3 10 7 12 4 19 20 17 21 9 0 16 15 18 24 23  & 57 & 216911529 & 149449 \\
30 & 3 5 12 1 0 11 6 18 14 4 10 9 17 2 8 16 7 23 13 19 15 20 21 22 24  & 54 & 291566548 & 2335437 \\
31 & 1 13 2 18 4 10 7 12 3 9 15 5 8 23 17 6 11 14 24 19 20 0 16 21 22  & 57 & 2171475074 & 32092322 \\
32 & 5 8 2 4 9 0 12 7 13 3 6 23 14 15 11 1 17 16 18 19 10 20 21 22 24  & 55 & 251342849 & 597705 \\
33 & 6 11 1 3 9 12 5 2 4 14 20 16 8 22 19 21 10 17 13 18 0 15 23 7 24  & 54 & 811584740 & 400051 \\
34 & 1 7 11 9 3 5 17 4 14 13 23 15 2 10 8 6 12 22 16 19 20 21 18 0 24  & 57 & 631980550 & 2448595 \\
35 & 4 5 3 9 18 2 6 8 14 24 10 16 7 12 13 15 1 23 11 22 0 20 21 19 17  & 62 & 2306723241 & 320354 \\
36 & 1 6 5 2 8 10 16 7 4 3 23 0 12 11 19 22 14 17 9 18 21 15 20 24 13  & 61 & 2397962096 & 2254318 \\
37 & 1 8 7 4 11 10 5 2 18 9 21 15 17 12 14 16 0 23 22 19 6 3 20 13 24  & 60 & 3105913297 & 30147361 \\
38 & 5 6 7 3 4 16 11 8 1 9 2 22 17 0 20 10 21 18 14 13 12 15 23 19 24  & 59 & 3972590436 & 10856193 \\
39 & 1 10 3 4 7 6 14 8 2 13 11 19 0 16 17 12 5 15 9 18 20 21 22 23 24  & 58 & 4222311744 & 9065785 \\
40 & 2 8 3 12 9 5 6 17 4 14 15 16 11 19 18 10 20 1 7 24 0 13 21 23 22  & 62 & 10471058020 & 3130786 \\
41 & 5 2 12 6 4 1 16 15 7 3 10 0 11 9 13 20 14 22 8 24 21 17 23 18 19  & 57 & 7857319738 & 790812 \\
42 & 1 8 5 4 9 10 6 7 3 2 11 20 21 24 12 0 15 18 13 14 17 16 23 22 19  & 59 & 6699839152 & 2192653 \\
43 & 1 5 14 7 4 2 18 6 12 19 11 0 8 23 3 15 10 22 13 9 20 16 17 21 24  & 59 & 10705654010 & 9617691 \\
44 & 8 3 5 9 4 10 11 2 6 14 16 17 21 19 18 1 0 7 12 13 22 15 20 23 24  & 62 & 8994638516 & 28070597 \\
45 & 5 4 2 7 18 6 17 11 1 3 10 0 13 9 8 15 20 16 19 23 22 21 24 14 12  & 63 & 21439293904 & 1528892 \\
46 & 5 1 2 18 13 10 6 3 12 9 11 19 17 21 4 22 7 20 24 14 16 15 0 23 8  & 64 & 28035575469 & 51697611 \\
47 & 5 10 2 8 9 6 12 4 18 14 1 21 17 7 24 13 0 22 11 23 15 16 20 3 19  & 64 & 24425377758 & 9888942 \\
48 & 11 6 1 9 3 7 2 12 4 13 5 0 15 8 14 10 20 19 22 18 17 23 16 21 24  & 61 & 41181598155 & 9787204 \\
49 & 6 10 11 7 4 0 5 17 8 1 15 2 12 9 19 20 13 16 14 3 22 18 21 23 24  & 63 & 47946481385 & 147577959 \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[]
\centering
\caption{マンハッタン距離をヒューリスティック関数に使用した際の各種アルゴリズムの実行時間}
\label{my-label}
\scalebox{0.7}{
\begin{tabular}{|c|c|c|c|}
\hline
問題ID &IDA*CPU&BPIDA*&BPIDA*GLOBAL\\ \hline
0 & 0.00109306 & 0.005179 & 0.003892 \\
1 & 0.000829162 & 0.004535 & 0.002827 \\
2 & 0.000114402 & 0.000146 & 0.000148 \\
3 & 4.5758e-05 & 0.000073 & 0.000073 \\
4 & 0.000475343 & 0.003984 & 0.002523 \\
5 & 0.0279696 & 0.038677 & 0.016160 \\
6 & 0.00181344 & 0.007498 & 0.004085 \\
7 & 0.0217618 & 0.032992 & 0.012678 \\
8 & 0.00228101 & 0.008179 & 0.004885 \\
9 & 0.018416 & 0.029385 & 0.011844 \\
10 & 0.128187 & 0.132161 & 0.043646 \\
11 & 0.182476 & 0.185138 & 0.057052 \\
12 & 0.136401 & 0.136927 & 0.042563 \\
13 & 0.0559939 & 0.062166 & 0.021708 \\
14 & 0.127124 & 0.127127 & 0.040839 \\
15 & 0.331212 & 0.304893 & 0.090014 \\
16 & 0.353343 & 0.335532 & 0.102163 \\
17 & 0.238382 & 0.227151 & 0.071430 \\
18 & 0.230989 & 0.220448 & 0.067561 \\
19 & 0.30467 & 0.286579 & 0.086292 \\
20 & 0.621227 & 0.554561 & 0.158965 \\
21 & 0.531318 & 0.472889 & 0.149421 \\
22 & 0.452306 & 0.426903 & 0.125053 \\
23 & 0.526157 & 0.464063 & 0.138359 \\
24 & 0.589896 & 0.513986 & 0.154375 \\
25 & 0.511253 & 0.475955 & 0.138158 \\
26 & 0.699993 & 0.623689 & 0.182011 \\
27 & 1.81112 & 1.565241 & 0.441174 \\
28 & 5.4267 & 4.417365 & 1.218770 \\
29 & 5.63523 & 4.752922 & 1.299228 \\
30 & 7.59373 & 6.127583 & 1.690318 \\
31 & 57.1595 & 46.197730 & 12.528994 \\
32 & 6.54183 & 5.460179 & 1.516923 \\
33 & 21.1905 & 17.049485 & 4.650572 \\
34 & 16.6193 & 13.491685 & 3.713547 \\
35 & 60.9398 & 50.106824 & 13.711796 \\
36 & 62.3192 & 51.745811 & 14.378452 \\
37 & 81.8274 & 65.365864 & 17.846566 \\
38 & 104.81 & 85.773459 & 23.467916 \\
39 & 111.616 & 87.380883 & 23.869678 \\
40 & 280.609 & 218.207440 & 59.604066 \\
41 & 208.087 & 167.020206 & 45.344936 \\
42 & 178.433 & 144.315918 & 39.024877 \\
43 & 281.667 & 224.980607 & 60.759933 \\
44 & 236.974 & 191.213256 & 52.417706 \\
45 & 572.981 & 459.664226 & 125.721762 \\
46 & 750.02 & 606.706851 & 164.160608 \\
47 & 648.048 & 527.484426 & 143.980162 \\
48 & 1081.36 & 867.153042 & 234.949619 \\
49 & 1263.04 & 1002.884366 & 273.728419 \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[]
\centering
\caption{異なる容量の配列を共有メモリに配置した際のBPIDA*GLOBALの実行時間}
\label{my-label}
\scalebox{0.7}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
問題ID &BPIDA*GLOBAL4KB&BPIDA*GLOBAL9KB&BPIDA*GLOBAL13KB&BPIDA*GLOBAL18KB&BPIDA*GLOBAL36KB\\ \hline
0 & 0.004656 & 0.009402 & 0.006079 & 0.007499 & 0.011695 \\
1 & 0.003360 & 0.006069 & 0.004662 & 0.005864 & 0.009756 \\
2 & 0.000148 & 0.000760 & 0.000147 & 0.000160 & 0.000202 \\
3 & 0.000076 & 0.000688 & 0.000072 & 0.000076 & 0.000089 \\
4 & 0.002968 & 0.003765 & 0.003947 & 0.004937 & 0.007933 \\
5 & 0.020099 & 0.029334 & 0.041775 & 0.059308 & 0.112757 \\
6 & 0.004957 & 0.006501 & 0.007778 & 0.010349 & 0.017718 \\
7 & 0.016109 & 0.024520 & 0.035349 & 0.050454 & 0.095891 \\
8 & 0.005032 & 0.006961 & 0.008502 & 0.011302 & 0.019364 \\
9 & 0.014072 & 0.021844 & 0.031258 & 0.044481 & 0.084071 \\
10 & 0.055746 & 0.093464 & 0.144152 & 0.211710 & 0.415011 \\
11 & 0.074519 & 0.129721 & 0.202778 & 0.298662 & 0.587059 \\
12 & 0.055701 & 0.096235 & 0.149525 & 0.220313 & 0.431972 \\
13 & 0.026862 & 0.045203 & 0.068301 & 0.099458 & 0.193079 \\
14 & 0.052666 & 0.090347 & 0.139775 & 0.205454 & 0.402751 \\
15 & 0.120417 & 0.214163 & 0.335335 & 0.497031 & 0.982160 \\
16 & 0.135580 & 0.237632 & 0.372256 & 0.549157 & 1.082723 \\
17 & 0.093098 & 0.162417 & 0.251874 & 0.370330 & 0.730425 \\
18 & 0.089200 & 0.155946 & 0.243520 & 0.360349 & 0.707451 \\
19 & 0.114576 & 0.202211 & 0.317410 & 0.469025 & 0.926326 \\
20 & 0.214539 & 0.385228 & 0.613427 & 0.911554 & 1.808540 \\
21 & 0.196539 & 0.338338 & 0.529274 & 0.779669 & 1.537018 \\
22 & 0.167350 & 0.297485 & 0.469483 & 0.695597 & 1.376134 \\
23 & 0.184306 & 0.327305 & 0.517337 & 0.765875 & 1.512784 \\
24 & 0.204545 & 0.360869 & 0.572617 & 0.848089 & 1.677985 \\
25 & 0.185663 & 0.330332 & 0.522745 & 0.774757 & 1.533419 \\
26 & 0.244205 & 0.434266 & 0.690646 & 1.023966 & 2.032421 \\
27 & 0.601745 & 1.088344 & 1.743941 & 2.596693 & 5.169114 \\
28 & 1.672602 & 3.046142 & 4.896203 & 7.300140 & 14.552291 \\
29 & 1.793026 & 3.275288 & 5.275193 & 7.868743 & 15.690678 \\
30 & 2.319815 & 4.234319 & 6.814901 & 10.159585 & 20.257245 \\
31 & 17.243572 & 31.637156 & 51.114712 & 76.336222 & 152.371878 \\
32 & 2.076190 & 3.783783 & 6.087070 & 9.076327 & 18.085252 \\
33 & 6.403943 & 11.744502 & 18.946642 & 28.284146 & 56.443835 \\
34 & 5.098057 & 9.321255 & 15.020809 & 22.405272 & 44.667472 \\
35 & 18.900297 & 34.687061 & 55.993126 & 83.597352 & 166.848708 \\
36 & 19.724242 & 35.981879 & 57.931395 & 86.353567 & 172.085250 \\
37 & 24.562682 & 45.074511 & 72.796044 & 108.694204 & 216.939940 \\
38 & 32.310296 & 59.239801 & 95.583596 & 142.671778 & 284.683837 \\
39 & 32.858946 & 60.214100 & 97.143899 & 145.046868 & 289.481732 \\
40 & 82.166738 & 150.882638 & 243.609770 & 363.731755 & 726.021751 \\
41 & 62.595419 & 115.124107 & 185.985502 & 277.759475 & 554.572450 \\
42 & 53.905648 & 99.230093 & 160.399004 & 239.613386 & 478.533097 \\
43 & 83.985267 & 154.321607 & 249.175294 & 372.026449 & 742.548049 \\
44 & 72.246958 & 132.388722 & 213.619611 & 318.817986 & 636.292447 \\
45 & 172.994574 & 317.177242 & 511.598897 & 763.510278 & 1524.347958 \\
46 & 226.728207 & 417.135236 & 674.310878 & 1007.368852 & 2011.693602 \\
47 & 198.421013 & 363.740102 & 587.089236 & 876.563159 & 1750.102250 \\
48 & 324.211623 & 595.655235 & 962.620400 & 1437.869799 & 2870.995589 \\
49 & 376.974507 & 692.174739 & 1117.882669 & 1669.696128 & 3333.789467 \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[]
\centering
\caption{PDBをヒューリスティック関数に使用した際の各種アルゴリズムの実行時間}
\label{my-label}
\scalebox{0.7}{
\begin{tabular}{|c|c|c|c|}
\hline
問題ID &IDA*CPU&BPIDA*&BPIDA*GLOBAL\\ \hline
0 & 0.000207838 & 0.000162 & 0.000176 \\
1 & 0.000367824 & 0.000290 & 0.000327 \\
2 & 4.9032e-05 & 0.000057 & 0.000066 \\
3 & 4.467e-05 & 0.000049 & 0.000048 \\
4 & 0.000191406 & 0.000172 & 0.000204 \\
5 & 0.00486043 & 0.011353 & 0.006868 \\
6 & 0.000932242 & 0.005709 & 0.003521 \\
7 & 0.000718589 & 0.000301 & 0.000321 \\
8 & 0.000539906 & 0.000380 & 0.000398 \\
9 & 0.00116524 & 0.005883 & 0.003577 \\
10 & 0.00215643 & 0.007021 & 0.004071 \\
11 & 0.0301163 & 0.035606 & 0.016215 \\
12 & 0.00813574 & 0.013437 & 0.006953 \\
13 & 0.000108296 & 0.000124 & 0.000129 \\
14 & 0.00428758 & 0.008679 & 0.004853 \\
15 & 0.00427699 & 0.008515 & 0.005527 \\
16 & 0.0160564 & 0.026041 & 0.011555 \\
17 & 0.000915544 & 0.004618 & 0.002854 \\
18 & 0.0121607 & 0.018171 & 0.008540 \\
19 & 0.00774828 & 0.014687 & 0.006842 \\
20 & 0.00555399 & 0.012041 & 0.005781 \\
21 & 0.0168652 & 0.021283 & 0.011546 \\
22 & 0.0178428 & 0.027157 & 0.011830 \\
23 & 0.016394 & 0.020262 & 0.009789 \\
24 & 0.00112212 & 0.004664 & 0.003026 \\
25 & 0.078412 & 0.070326 & 0.028499 \\
26 & 0.0111169 & 0.017911 & 0.008242 \\
27 & 0.0120896 & 0.020293 & 0.009419 \\
28 & 0.144657 & 0.100431 & 0.043395 \\
29 & 0.0142936 & 0.020651 & 0.010444 \\
30 & 0.224169 & 0.159144 & 0.062378 \\
31 & 2.96813 & 1.408887 & 0.546401 \\
32 & 0.0559264 & 0.049767 & 0.021000 \\
33 & 0.0375624 & 0.043436 & 0.018810 \\
34 & 0.22336 & 0.141301 & 0.060174 \\
35 & 0.0308752 & 0.036840 & 0.016075 \\
36 & 0.203351 & 0.135135 & 0.056161 \\
37 & 2.72553 & 1.262451 & 0.504489 \\
38 & 1.06718 & 0.541771 & 0.213354 \\
39 & 0.852431 & 0.418638 & 0.169901 \\
40 & 0.308254 & 0.189456 & 0.075241 \\
41 & 0.0755763 & 0.062189 & 0.025706 \\
42 & 0.204659 & 0.123832 & 0.059423 \\
43 & 0.910621 & 0.451724 & 0.179226 \\
44 & 2.65879 & 1.269991 & 0.495308 \\
45 & 0.144441 & 0.101844 & 0.044034 \\
46 & 5.12469 & 2.251534 & 0.876291 \\
47 & 1.01127 & 0.481867 & 0.192283 \\
48 & 0.966463 & 0.482727 & 0.198767 \\
49 & 13.9245 & 6.128629 & 2.348042 \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[]
\centering
\caption{ヒューリスティック関数をマンハッタン距離から、PDBに変更した際の各種アルゴリズムの高速化の割合}
\label{my-label}
\scalebox{0.7}{
\begin{tabular}{|c|c|c|c|}
\hline
問題ID &cpuPDBspeedup&bpida*PDBspeedup&bpida*globalPDBspeedup\\ \hline
0 & 5.25919 & 31.012 & 21.5028 \\
1 & 2.25424 & 15.5842 & 11.043 \\
2 & 2.33321 & 2.35484 & 2.42623 \\
3 & 1.02436 & 1.35185 & 1.32727 \\
4 & 2.48343 & 21.5351 & 13.9392 \\
5 & 5.75455 & 3.32705 & 2.1561 \\
6 & 1.94525 & 1.28765 & 1.12318 \\
7 & 30.2841 & 113.375 & 40.6346 \\
8 & 4.22483 & 21.0257 & 13.7606 \\
9 & 15.8045 & 4.94032 & 3.18216 \\
10 & 59.4441 & 18.5437 & 10.5349 \\
11 & 6.05904 & 5.10881 & 3.36789 \\
12 & 16.7657 & 9.9417 & 5.65997 \\
13 & 517.045 & 453.766 & 136.528 \\
14 & 29.6494 & 14.4742 & 7.97793 \\
15 & 77.4404 & 35.5063 & 15.3634 \\
16 & 22.0064 & 12.6501 & 8.38708 \\
17 & 260.372 & 47.1268 & 22.6978 \\
18 & 18.9947 & 11.949 & 7.40638 \\
19 & 39.321 & 19.1308 & 11.314 \\
20 & 111.852 & 45.2703 & 25.1965 \\
21 & 31.5038 & 21.6951 & 12.3122 \\
22 & 25.3495 & 15.1966 & 9.91618 \\
23 & 32.0945 & 22.1912 & 13.0037 \\
24 & 525.698 & 106.548 & 46.5125 \\
25 & 6.52009 & 6.62383 & 4.65586 \\
26 & 62.9666 & 34.1243 & 20.55 \\
27 & 149.808 & 76.0786 & 43.3118 \\
28 & 37.5143 & 43.587 & 27.0958 \\
29 & 394.248 & 222.933 & 116.21 \\
30 & 33.875 & 37.8732 & 26.1732 \\
31 & 19.2577 & 32.7645 & 22.6671 \\
32 & 116.972 & 107.56 & 69.1585 \\
33 & 564.141 & 384.136 & 231.856 \\
34 & 74.4059 & 94.7423 & 60.0936 \\
35 & 1973.75 & 1334.51 & 821.508 \\
36 & 306.461 & 378.58 & 251.666 \\
37 & 30.0226 & 51.7878 & 35.0025 \\
38 & 98.2121 & 156.961 & 108.273 \\
39 & 130.938 & 208.119 & 138.755 \\
40 & 910.317 & 1142.11 & 780.628 \\
41 & 2753.34 & 2637.76 & 1718.2 \\
42 & 871.855 & 1155.05 & 648.771 \\
43 & 309.313 & 497.609 & 333.165 \\
44 & 89.1285 & 150.646 & 105.145 \\
45 & 3966.89 & 4432.16 & 2797.55 \\
46 & 146.354 & 267.141 & 186.046 \\
47 & 640.826 & 1095.31 & 738.955 \\
48 & 1118.88 & 1786.13 & 1162.22 \\
49 & 90.7063 & 163.144 & 116.033 \\ \hline
\end{tabular}
}
\end{table}

\end{document}
